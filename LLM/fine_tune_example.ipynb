{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bitsandbytes\n",
    "# accelerate \n",
    "# loralib\n",
    "!pip install -q -U torch\n",
    "!pip install -q -U transformers\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(id: str) -> tuple:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        id,\n",
    "        load_in_8bit=True,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(id)\n",
    "    return model, tokenizer\n",
    "\n",
    "def generate_text(prompt: str, model: AutoModelForCausalLM, tokenizer:AutoTokenizer, max_new_tokens:int=20) -> str:\n",
    "    batch = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "      output_tokens = model.generate(**batch, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    return tokenizer.decode(output_tokens[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Evaluate base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(id=\"databricks/dolly-v2-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"What is your name?\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"Tell me your name!\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply LoRA preprocessing to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"LORA_R\": 16,\n",
    "    \"LORA_ALPHA\": 32,\n",
    "    \"LORA_DROPOUT\": 0.05,\n",
    "    \"PER_DEVICE_TRAIN_BATCH_SIZE\": 4,\n",
    "    \"GRADIENT_ACCUMULATION_STEPS\": 4,\n",
    "    \"WARMUP_STEPS\": 100,\n",
    "    \"MAX_STEPS\": 40,\n",
    "    \"LEARNING_RATE\": 0.0002\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "# Parameter freezing\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "model.enable_input_require_grads()\n",
    "model.embed_out = CastOutputToFloat(model.embed_out)\n",
    "# LORA\n",
    "config = LoraConfig(\n",
    "    r=config[\"LORA_R\"],\n",
    "    lora_alpha=config[\"LORA_ALPHA\"],\n",
    "    lora_dropout=config[\"LORA_DROPOUT\"],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "# Add Low Rank Adapters + freezing\n",
    "model = get_peft_model(model, config)\n",
    "trainable_params = 0\n",
    "all_param = 0\n",
    "for _, param in model.named_parameters():\n",
    "    all_param += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "\n",
    "print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"{data_point[\"instruction\"]}\n",
    "\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    data = load_dataset(\"json\", data_files=\"./data/chris_train.json\")\n",
    "\n",
    "    data = data.shuffle().map(\n",
    "        lambda data_point: tokenizer(\n",
    "            generate_prompt(data_point),\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "    )\n",
    "    return data\n",
    "\n",
    "data = load_data()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define & Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=data[\"train\"],\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=config[\"PER_DEVICE_TRAIN_BATCH_SIZE\"],\n",
    "            gradient_accumulation_steps=config[\"GRADIENT_ACCUMULATION_STEPS\"],\n",
    "            warmup_steps=config[\"WARMUP_STEPS\"],\n",
    "            max_steps=config[\"MAX_STEPS\"],\n",
    "            learning_rate=config[\"LEARNING_RATE\"],\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=\"outputs\",\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=False\n",
    "        ),\n",
    "    )\n",
    "model.config.use_cache = (\n",
    "        False  # silence the warnings. Please re-enable for inference!\n",
    "    )\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"What is your name?\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"Tell me your name!\", model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
